{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57518f4b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CVAE类\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, latent_dim, activation_function):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoders = nn.ModuleList()\n",
    "        in_dim = input_dim + 2\n",
    "        for h_dim in hidden_sizes:\n",
    "            self.encoders.append(nn.Linear(in_dim, h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_sizes[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_sizes[-1], latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoders = nn.ModuleList()\n",
    "        in_dim = latent_dim + 2\n",
    "        for h_dim in reversed(hidden_sizes):\n",
    "            self.decoders.append(nn.Linear(in_dim, h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_sizes[0], input_dim)\n",
    "\n",
    "    def encoder(self, x, condition):\n",
    "        x = torch.cat((x, condition), dim=1)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x)\n",
    "            x = self.get_activation(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decoder(self, z, condition):\n",
    "        z = torch.cat((z, condition), dim=1)\n",
    "        for layer in self.decoders:\n",
    "            z = layer(z)\n",
    "            z = self.get_activation(z)\n",
    "        return self.fc_out(z)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encoder(x, condition)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z, condition), mu, logvar\n",
    "\n",
    "    def get_activation(self, x):\n",
    "        if self.activation_function == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation_function == 'leaky_relu':\n",
    "            return F.leaky_relu(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return 2*MSE + KLD, MSE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(dataset, input_dim, device, n_iter=20, epochs=1000):\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # # 随机选择超参数\n",
    "        # hidden_layers = np.random.choice(range(2, 6))\n",
    "        # hidden_sizes = [np.random.choice([50, 100, 150, 200, 250]) for _ in range(hidden_layers)]\n",
    "        # learning_rate = np.random.choice([1e-5, 1e-4, 1e-3, 1e-2])\n",
    "        # latent_dim = np.random.choice([2, 4, 8, 16, 32])\n",
    "        # batch_size = np.random.choice([32, 64, 128, 256, 512, 1024, 2048])\n",
    "        # # activation_function = np.random.choice(['relu', 'tanh', 'sigmoid', 'leaky_relu'])\n",
    "        # activation_function = np.random.choice(['relu'])\n",
    "\n",
    "        hidden_layers = 2\n",
    "        hidden_sizes = [200, 100]\n",
    "        learning_rate = 0.001\n",
    "        latent_dim = 4\n",
    "        activation_function = 'relu'\n",
    "        # batch_size = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "        batch_size = [512]\n",
    "        batch_size = batch_size[i]\n",
    "\n",
    "        # 打印调试信息\n",
    "        print(f'Iteration [{i + 1}/{n_iter}], Hidden sizes: {hidden_sizes}, Learning rate: {learning_rate}, Latent dim: {latent_dim}, Batch size: {batch_size}, Activation function: {activation_function}')\n",
    "\n",
    "        # 创建模型\n",
    "        cvae = CVAE(input_dim=input_dim, hidden_sizes=hidden_sizes, latent_dim=latent_dim,\n",
    "                  activation_function=activation_function).to(device)\n",
    "        optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "        # 创建训练集和测试集\n",
    "        train_data, test_data = train_test_split(dataset, test_size=0.01, random_state=42)\n",
    "\n",
    "        # 创建训练DataLoader\n",
    "        batch_size = int(batch_size)\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # 训练模型\n",
    "        cvae_train_loss = []\n",
    "        cvae_test_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            cvae.train()\n",
    "            for train_batch_data in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                recon, mu, logvar = cvae(train_batch_data[:, :199], train_batch_data[:, 199:201])\n",
    "                loss, mse_loss, kld_loss = loss_function(recon, train_batch_data[:, :199], mu, logvar)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            recon, mu, logvar = cvae(train_data[:, :199], train_data[:, 199:201])\n",
    "            train_loss = F.mse_loss(recon, train_data[:, :199])\n",
    "            cvae_train_loss.append(train_loss.item())\n",
    "\n",
    "            # 验证模型\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                test_recon, mu, logvar = cvae(test_data[:, :199], test_data[:, 199:201])\n",
    "                test_loss = F.mse_loss(test_recon, test_data[:, :199])\n",
    "                cvae_test_loss.append(test_loss.item())\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f'Epoch [{epoch + 1}/{epochs}], train_loss: {train_loss.item():.8f}, test_loss: {test_loss.item():.8f}')\n",
    "\n",
    "            # 保存模型\n",
    "            torch.save(cvae.state_dict(), 'best_cvae.pth')\n",
    "            np.savetxt('cvae_train_loss.dat', cvae_train_loss, delimiter='\\t')\n",
    "            np.savetxt('cvae_test_loss.dat', cvae_test_loss, delimiter='\\t')\n",
    "\n",
    "    return best_params, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train():\n",
    "    # 准备训练数据\n",
    "    data = np.loadtxt('airfoils_recon_data.dat')\n",
    "    # 剔除样本太少的数据\n",
    "    data = data[(data[:, 199] > 0.02) & (data[:, 199] < 0.3)]  # thickness\n",
    "    data = data[(data[:, 200] > 0.5) & (data[:, 200] < 1.5)]  # cl\n",
    "    data = data[(data[:, 201] > 0.01)]  # cd\n",
    "    data = data[(data[:, 202] > 15) & (data[:, 202] < 65)]  # ld\n",
    "\n",
    "    airfoil_condition = data[:, :201]\n",
    "    airfoil_condition_min = np.min(airfoil_condition, axis=0)\n",
    "    airfoil_condition_max = np.max(airfoil_condition, axis=0)\n",
    "    airfoil_condition_nom = (airfoil_condition - airfoil_condition_min) / (airfoil_condition_max - airfoil_condition_min)\n",
    "    dataset = torch.FloatTensor(airfoil_condition_nom).to(device)\n",
    "\n",
    "    # 执行随机搜索\n",
    "    best_params, best_loss = random_search(dataset, input_dim=199, device=device, n_iter=1, epochs=1000)\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    main_train()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
